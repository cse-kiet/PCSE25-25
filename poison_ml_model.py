# -*- coding: utf-8 -*-
"""Poison_ML_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/DeepanshuMishra49/Poision-ML_Model/blob/main/Poison_ML_model.ipynb
"""

import pandas as pd
#Load the dataset
file_path = "/content/Heart Attack.csv"
data = pd.read_csv(file_path)

data.head()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Set a random seed for reproducibility
RANDOM_SEED = 123
np.random.seed(RANDOM_SEED)

# Separate features and target
target_column = "class"
X = data.drop(columns=[target_column])
y = data[target_column]

# Encode categorical features
categorical_cols = X.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Encode target labels (i.e., "positive" -> 1, "negative" -> 0)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED)

# Poisoning rates
poison_rates = [0, 5, 10, 20]

# Models with fixed random states
models = {
    "SVM": SVC(random_state=RANDOM_SEED),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),
    "Random Forest": RandomForestClassifier(random_state=RANDOM_SEED),
    "Decision Tree": DecisionTreeClassifier(random_state=RANDOM_SEED),
}

# Store results
results = {model_name: {"clean": None, "poisoned": {rate: None for rate in poison_rates}} for model_name in models}

# Evaluate models on clean data (0% poisoning)
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[model_name]["clean"] = accuracy_score(y_test, y_pred)

# Evaluate models on poisoned data
for rate in poison_rates:
    num_poisoned = int((rate / 100) * len(y_train))
    poisoned_y_train = y_train.copy()
    if num_poisoned > 0:
        poison_indices = np.random.choice(len(y_train), num_poisoned, replace=False)
        for idx in poison_indices:
            original_label = poisoned_y_train[idx]
            other_labels = [label for label in np.unique(y) if label != original_label]
            poisoned_y_train[idx] = np.random.choice(other_labels)

    # Train and evaluate each model on poisoned data
    for model_name, model in models.items():
        model.fit(X_train, poisoned_y_train)
        y_pred = model.predict(X_test)
        results[model_name]["poisoned"][rate] = accuracy_score(y_test, y_pred)

# Display results
print("\nAccuracy Results (Before and After Poisoning):")
for model_name, model_results in results.items():
    print(f"\n{model_name}:")
    for rate, acc in model_results["poisoned"].items():
        print(f"  Accuracy after {rate}% poisoning: {acc:.2f}")

# Plot results
plt.figure(figsize=(10, 6))
for model_name, model_results in results.items():
    accuracies = [model_results["clean"]] + [model_results["poisoned"][rate] for rate in poison_rates]
    plt.plot([0] + poison_rates, accuracies, marker='o', label=model_name)

plt.title("Model Accuracy vs. Poisoning Rate")
plt.xlabel("Poisoning Rate (%)")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.show()